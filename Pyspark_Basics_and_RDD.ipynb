{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bb0c09",
   "metadata": {},
   "source": [
    "## Learning Pyspark from Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63bdd5f",
   "metadata": {},
   "source": [
    "#### Installing PySpark and findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6f9add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\sahil kale\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\sahil kale\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3bf189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\sahil kale\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: pyspark in c:\\users\\sahil kale\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\sahil kale\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "# Mandatory step so that jupyter notebook finds your pyspark\n",
    "! pip install findspark pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff56e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('F:\\CDAC\\Big Data\\pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ab523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "# mandatory step\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7fcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2530adb4",
   "metadata": {},
   "source": [
    "## Understanding SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dd1be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing SparkContext - Entry point for spark\n",
    "\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87d1b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mandaatory step\n",
    "# we are setting spark context to local mode and appName as - 'understanding Spark Context'\n",
    "\n",
    "sc = SparkContext(master = 'local[4]', appName = 'Understanding Spark Context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e32f6163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://SAHIL:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Understanding Spark Context</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=Understanding Spark Context>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8e9994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1712388061787"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints start time of the SparkContext Session\n",
    "\n",
    "sc.startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1cf230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the current version of SparkContext\n",
    "\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69a7a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the python version of SparkContext \n",
    "\n",
    "sc.pythonVer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8220f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints where our Spark is running\n",
    "\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b7ef6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkContext.getConf of <SparkContext master=local appName=Understanding Spark Context>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetching the configurations of SparkContext\n",
    "\n",
    "sc.getConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac44a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01ac1bab",
   "metadata": {},
   "source": [
    "# RDD - RESILIENT DISTRUBUTED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "711085bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallelize returns:  <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Loading the data in Pyspark from local data structure\n",
    "\n",
    "# Parallelize method is used to create RDD from local data structure(here-list)\n",
    "\n",
    "numRDD = sc.parallelize([1,2,3,4,5])\n",
    "print(\"Parallelize returns: \", type(numRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5caa27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text File returns:  <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Loading the data in Pyspark from text file\n",
    "\n",
    "# if full file:/// with path is provided if loads file from *LOCAL FILE SYSTEM*\n",
    "# if only file is specified it  finds the file in *HADOOP FILE SYSTEM*\n",
    "# (here-local)\n",
    "\n",
    "fileRDD = sc.textFile('file:///F:\\CDAC\\Big Data\\pyspark\\test.txt')\n",
    "print(\"Text File returns: \", type(fileRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da73a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f10b392c",
   "metadata": {},
   "source": [
    "##### Understanding partitoning in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83689be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for parallelize method\n",
    "# create RDD from 0-9 with partitions(numSlices) -> 6\n",
    "\n",
    "numRDD = sc.parallelize(range(10), numSlices = 6)\n",
    "type(numRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e16909e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for text file method\n",
    "# create RDD from given file in HDFS with minimum partitions -> 3 \n",
    "\n",
    "fileRDD = sc.textFile('test.txt', minPartitions = 3)\n",
    "type(fileRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbacee5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e71d84f8",
   "metadata": {},
   "source": [
    "#### Get Number of partitions of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27264de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions of numRDD1 are:  6\n"
     ]
    }
   ],
   "source": [
    "# this method return number of partitions \n",
    "\n",
    "print(\"Number of partitions of numRDD1 are: \", numRDD.getNumPartitions() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ef21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23f2b307",
   "metadata": {},
   "source": [
    "#### Display contents of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "533a3e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output without partitions:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# display contents of RDD\n",
    "# returns list and show O/P without partitions\n",
    "\n",
    "print(\"Output without partitions: \", numRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "919e34df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with partitions:  [[0], [1, 2], [3, 4], [5], [6, 7], [8, 9]]\n"
     ]
    }
   ],
   "source": [
    "# display contents of RDD \n",
    "# *BUT*\n",
    "# returns list, with *partitions* it was stored with\n",
    "\n",
    "print(\"Output with partitions: \", numRDD.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46d48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a78c9231",
   "metadata": {},
   "source": [
    "## TRANSFORMATION - ACTION on RDDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06edbe4",
   "metadata": {},
   "source": [
    "### Basic RDD Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e90fe",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af71e0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map will iterate through each element in the list and perform the operation for each element\n",
    "# will return O/P in collection of collection\n",
    "\n",
    "rdd = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "rdd_map = rdd.map(lambda x: x.split(\" \"))\n",
    "\n",
    "rdd_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b66cd",
   "metadata": {},
   "source": [
    "### FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1edd1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter will iterate through each element and return elements which give True for given condition\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8])\n",
    "rdd_filter = rdd.filter(lambda x : x%2 ==0)\n",
    "\n",
    "rdd_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce615f6",
   "metadata": {},
   "source": [
    "### FLATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac0120ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as map but will return O/P in single collection\n",
    "\n",
    "rdd = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "rdd_flatmap = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "rdd_flatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6ef52",
   "metadata": {},
   "source": [
    "### UNION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "173f8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines 2 or more RDD\n",
    "\n",
    "error_rdd = sc.parallelize(['error1', 'error2'])\n",
    "warning_rdd = sc.parallelize(['warning1', 'warning2'])\n",
    "\n",
    "rdd_union = error_rdd.union(warning_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b67cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70ddef34",
   "metadata": {},
   "source": [
    "### Basic RDD Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8edbb",
   "metadata": {},
   "source": [
    "### COLLECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e16538f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map vs Flatmap O/P: \n",
      "Map output: [['hello', 'world'], ['how', 'are', 'you']]\n",
      "Flatmap output: ['hello', 'world', 'how', 'are', 'you'] \n",
      "\n",
      "Filter output: [2, 4, 6, 8]\n",
      "Union output: ['error1', 'error2', 'warning1', 'warning2']\n"
     ]
    }
   ],
   "source": [
    "# collect return all elements of dataset in ARRAY\n",
    "\n",
    "print(\"Map vs Flatmap O/P: \")\n",
    "print(\"Map output:\", rdd_map.collect())\n",
    "print(\"Flatmap output:\", rdd_flatmap.collect() ,\"\\n\")\n",
    "print(\"Filter output:\", rdd_filter.collect())\n",
    "print(\"Union output:\", rdd_union.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f4dd6",
   "metadata": {},
   "source": [
    "### TAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09f2bdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map vs Flatmap O/P: \n",
      "Map output:  [['hello', 'world']]\n",
      "FLatmap output:  ['hello'] \n",
      "\n",
      "Filter output:  [2, 4, 6]\n",
      "Union output:  ['error1']\n"
     ]
    }
   ],
   "source": [
    "# take returns specified first n number of elements in array\n",
    "\n",
    "print(\"Map vs Flatmap O/P: \")\n",
    "print(\"Map output: \", rdd_map.take(1))\n",
    "print(\"FLatmap output: \", rdd_flatmap.take(1), \"\\n\")\n",
    "print(\"Filter output: \", rdd_filter.take(3))\n",
    "print(\"Union output: \", rdd_union.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ac040",
   "metadata": {},
   "source": [
    "### FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba3221e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map vs Flatmap O/P: \n",
      "Map output:  ['hello', 'world']\n",
      "FLatmap output:  hello \n",
      "\n",
      "Filter output:  2\n",
      "Union output:  error1\n"
     ]
    }
   ],
   "source": [
    "# first prints the first element of collection\n",
    "\n",
    "print(\"Map vs Flatmap O/P: \")\n",
    "print(\"Map output: \", rdd_map.first())\n",
    "print(\"FLatmap output: \", rdd_flatmap.first(), \"\\n\")\n",
    "print(\"Filter output: \", rdd_filter.first())\n",
    "print(\"Union output: \", rdd_union.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7980e4f",
   "metadata": {},
   "source": [
    "### COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38ebe0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map vs Flatmap O/P: \n",
      "Map output:  2\n",
      "FLatmap output:  5 \n",
      "\n",
      "Filter output:  4\n",
      "Union output:  4\n"
     ]
    }
   ],
   "source": [
    "# count returns the count of elements in collection\n",
    "\n",
    "print(\"Map vs Flatmap O/P: \")\n",
    "print(\"Map output: \", rdd_map.count())\n",
    "print(\"FLatmap output: \", rdd_flatmap.count(), \"\\n\")\n",
    "print(\"Filter output: \", rdd_filter.count())\n",
    "print(\"Union output: \", rdd_union.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5103d61c",
   "metadata": {},
   "source": [
    "### REDUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19128faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregates the elements from regular RDD\n",
    "\n",
    "rdd = sc.parallelize([2,3,7,8])\n",
    "# passing a function as parameter is compulsory\n",
    "rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a997125",
   "metadata": {},
   "source": [
    "### SAVE AS TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "311826aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default saveAsTextFile saves into text file in partitions it was created with\n",
    "\n",
    "# rdd_flatmap.saveAsTextFile(\"Basic_rdd_operations.txt\")\n",
    "\n",
    "# if used coalesce you can give number of partitions you want for the text file\n",
    "# coalesce is TRANSFORMATION\n",
    "\n",
    "# rdd_flatmap.coalesce(1).saveAsTextFile(\"Basic_rdd_operations_using_coalesce.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2c554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f040585",
   "metadata": {},
   "source": [
    "## PAIRED RDD - Key, Value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e406f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sahil', 42), ('Omkar', 34), ('Swarada', 34)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating key value pair using tuple in list\n",
    "\n",
    "paired_rdd_tuple = sc.parallelize( [('Sahil', 42), ('Omkar', 34), ('Swarada', 34)] )\n",
    "paired_rdd_tuple.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec68c2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sahil', '42'), ('Omkar', '34'), ('Swarada', '34')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way of creating paired RDD\n",
    "# extract tuple from list using map\n",
    "\n",
    "lst = ['Sahil 42', 'Omkar 34', 'Swarada 34']\n",
    "rdd_list = sc.parallelize(lst)\n",
    "# using map to split string in tuple 0th index- key, 1st index- value \n",
    "paired_rdd_list = rdd_list.map(lambda x : (x.split(' ')[0], x.split(' ')[1]) )\n",
    "paired_rdd_list.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8b3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "104e8c7f",
   "metadata": {},
   "source": [
    "### TRANSFORMATIONS ON PAIRED RDD\n",
    "All basic RDD transformations along with additional below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffc3d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairedRDD = sc.parallelize( [ ('A',1), ('B',2), ('C',1), ('A',9), ('A', 11), ('C', 7) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df821be",
   "metadata": {},
   "source": [
    "### REDUCE BY KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f331c861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 21), ('B', 2), ('C', 8)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey reduces the K,V based on keys and perform operation for values that match a key\n",
    "# the below lambda function sums up the values for same keys\n",
    "\n",
    "pairedRDD_reduceByKey = pairedRDD.reduceByKey(lambda x,y : x + y)\n",
    "pairedRDD_reduceByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51acbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a518739a",
   "metadata": {},
   "source": [
    "### GROUP BY KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c48b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', [1, 9, 11])\n",
      "('B', [2])\n",
      "('C', [1, 7])\n"
     ]
    }
   ],
   "source": [
    "# groupByKey groups the values based on key\n",
    "\n",
    "pairedRDD_groupByKey = pairedRDD.groupByKey().collect()\n",
    "for k,v in pairedRDD_groupByKey:\n",
    "    print( (k, list(v)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d7f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39c6e7c1",
   "metadata": {},
   "source": [
    "### SORT BY KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcae4380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascending sort:  [('A', 21), ('B', 2), ('C', 8)]\n",
      "Descending sort: [('C', 8), ('B', 2), ('A', 21)]\n"
     ]
    }
   ],
   "source": [
    "# sortByKey sorts the collection in ascending order of key by default\n",
    "\n",
    "pairedRDD_sortByKey = pairedRDD_reduceByKey.sortByKey()\n",
    "print(\"Ascending sort: \", pairedRDD_sortByKey.collect())\n",
    "\n",
    "# sorting in descending order\n",
    "\n",
    "pairedRDD_sortByKey_desc = pairedRDD_reduceByKey.sortByKey(ascending = False)\n",
    "print(\"Descending sort:\", pairedRDD_sortByKey_desc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c7df1",
   "metadata": {},
   "source": [
    "### JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b9676a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (12, 99)), ('B', (20, 1)), ('C', (1, 4))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joins 2 RDD's on the basis of key\n",
    "\n",
    "rdd1 = sc.parallelize( [('A',12), ('B',20), ('C',1)] )\n",
    "rdd2 = sc.parallelize( [('B',1), ('A',99), ('C',4)] )\n",
    "pairedRDD_join = rdd1.join(rdd2)\n",
    "pairedRDD_join.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419a36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "604cdb24",
   "metadata": {},
   "source": [
    "### ACTIONS ON PAIRED RDD\n",
    "All basic RDD actions along with additional below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e7e2b",
   "metadata": {},
   "source": [
    "### COUNT BY KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7470feb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'A': 3, 'B': 1, 'C': 2})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countByKey returns count of elements for each key\n",
    "\n",
    "pairedRDD.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98042bb",
   "metadata": {},
   "source": [
    "### COLLECT AS MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47291128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 12, 'B': 20, 'C': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collectAsMap returns paired RDD as 'Dictionary'\n",
    "\n",
    "rdd1.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3177aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
